{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import mpld3\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "data_dir = '../data/azsecure'\n",
    "model_dir = '../models/'\n",
    "data = {\n",
    "    'ansar1': {'path': os.path.join(data_dir, 'Ansar1.txt.gz'), 'delimiter': '\\t'},\n",
    "    'hackhound': {'path': os.path.join(data_dir, 'Hackhound.csv.gz'), 'delimiter': ','},\n",
    "    'islamic_awakening': {'path': os.path.join(data_dir, 'IslamicAwakening.txt.gz'), 'delimiter': '\\t'},\n",
    "    'islamic_network': {'path': os.path.join(data_dir, 'IslamicNetwork.txt.gz'), 'delimiter': '\\t'},\n",
    "}\n",
    "sites = sorted(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dataframe(site):\n",
    "    \"\"\"\n",
    "    Return Pandas Dataframe from site tsv/csv.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data[site]['path'], header=0, sep=data[site]['delimiter'],\n",
    "                     error_bad_lines=False, warn_bad_lines=True)\n",
    "    return df\n",
    "\n",
    "def get_text_blob(df):\n",
    "    \"\"\"\n",
    "    Concatenate all text in df 'Message' column into a blob of text.\n",
    "    \"\"\"\n",
    "    blob = ''\n",
    "    for index, row in df['Message'].iteritems():\n",
    "        blob += '\\n{}'.format(row)\n",
    "    return blob \n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = nltk.wordpunct_tokenize(text)\n",
    "    clean_tokens = [w.lower() for w in tokens if w.isalpha()]\n",
    "    stems = [stemmer.stem(token) for token in clean_tokens]\n",
    "    return stems\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.wordpunct_tokenize(text)\n",
    "    clean_tokens = [w.lower() for w in tokens if w.isalpha()]\n",
    "    return clean_tokens\n",
    "\n",
    "def save_model(model, name):\n",
    "    \"\"\"\n",
    "    Pickle model in models directory using joblib.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(model_dir, name)\n",
    "    joblib.dump(model, filepath)\n",
    "    \n",
    "def load_model(name):\n",
    "    \"\"\"\n",
    "    Get pickled model `name` from model directory\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(model_dir, name)\n",
    "    return joblib.load(filepath)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Corpus Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 29064: expected 11 fields, saw 12\\nSkipping line 29065: expected 11 fields, saw 12\\nSkipping line 29071: expected 11 fields, saw 12\\nSkipping line 29072: expected 11 fields, saw 12\\nSkipping line 29078: expected 11 fields, saw 12\\nSkipping line 29338: expected 11 fields, saw 12\\nSkipping line 29428: expected 11 fields, saw 12\\nSkipping line 29436: expected 11 fields, saw 12\\nSkipping line 29438: expected 11 fields, saw 12\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Built dataframe for ansar1\n",
      "[*] Built text blob for ansar1\n",
      "[*] Built word stems for ansar1\n",
      "[*] Built word tokens for ansar1\n",
      "[*] Built dataframe for hackhound\n",
      "[*] Built text blob for hackhound\n",
      "[*] Built word stems for hackhound\n",
      "[*] Built word tokens for hackhound\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 95059: expected 11 fields, saw 14\\n'\n",
      "b'Skipping line 138730: expected 11 fields, saw 14\\nSkipping line 184781: expected 11 fields, saw 14\\n'\n",
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Built dataframe for islamic_awakening\n",
      "[*] Built text blob for islamic_awakening\n",
      "[*] Built word stems for islamic_awakening\n",
      "[*] Built word tokens for islamic_awakening\n",
      "[*] Built dataframe for islamic_network\n",
      "[*] Built text blob for islamic_network\n",
      "[*] Built word stems for islamic_network\n",
      "[*] Built word tokens for islamic_network\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Aim here is index of stems-words used across all the sites.\n",
    "\n",
    "total_words = []\n",
    "total_stems = []\n",
    "for site in sites:\n",
    "    df = get_dataframe(site)\n",
    "    print('[*] Built dataframe for {}'.format(site))\n",
    "    text = get_text_blob(df)\n",
    "    data[site]['text'] = text\n",
    "    print('[*] Built text blob for {}'.format(site))\n",
    "    stems = tokenize_and_stem(text)\n",
    "    print('[*] Built word stems for {}'.format(site))\n",
    "    words = tokenize(text)\n",
    "    print('[*] Built word tokens for {}'.format(site))\n",
    "    total_stems.extend(stems)\n",
    "    total_words.extend(words)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 51235079 words in the vocabulary used across all sites.\n"
     ]
    }
   ],
   "source": [
    "vocab_df = pd.DataFrame({'words': total_words}, index = total_stems)\n",
    "print('There are {} words in the vocabulary used across all sites.'.format(vocab_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Site Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 11s, sys: 12 s, total: 13min 23s\n",
      "Wall time: 13min 48s\n",
      "(4, 200000)\n"
     ]
    }
   ],
   "source": [
    "site_texts = [data[site]['text'] for site in sites]\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(site_texts) #fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.22044605e-15,   9.99899296e-01,   6.02882248e-01,\n",
       "          6.77900459e-01],\n",
       "       [  9.99899296e-01,   0.00000000e+00,   9.99761427e-01,\n",
       "          9.99601286e-01],\n",
       "       [  6.02882248e-01,   9.99761427e-01,  -2.22044605e-15,\n",
       "          1.22180109e-01],\n",
       "       [  6.77900459e-01,   9.99601286e-01,   1.22180109e-01,\n",
       "         -1.11022302e-15]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.05 s, sys: 16 ms, total: 4.06 s\n",
      "Wall time: 4.11 s\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 2\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_model(km, 'azsecure.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "km = load_model('azsecure.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "site_matrix = {'site': sites, 'text': site_texts, 'cluster': clusters}\n",
    "sites_df = pd.DataFrame(site_matrix, index=[clusters])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3\n",
       "0    1\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "\n",
      "['hackhound']\n",
      "\n",
      "Cluster 1:\n",
      "\n",
      "['ansar1', 'islamic_awakening', 'islamic_network']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_clusters):\n",
    "    print('Cluster {}:'.format(i))\n",
    "    print()\n",
    "    print(sites_df.loc[sites_df['cluster'] == i]['site'].values.tolist())\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: ff, ff, eax, dword, mov, ptr,\n",
      "\n",
      "Cluster 1 words: allah, muslim, islamic, م, في, wa,\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        #print(' %s' % vocab_df.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "        print(' %s' % vocab_df.ix[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
